<!DOCTYPE html>
<html lang="zh-cn">
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
  <title>强化学习面试准备 - SongLei&#39;s Blog</title>
  <meta name="renderer" content="webkit" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>

<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />

<meta name="theme-color" content="#f8f5ec" />
<meta name="msapplication-navbutton-color" content="#f8f5ec">
<meta name="apple-mobile-web-app-capable" content="yes">
<meta name="apple-mobile-web-app-status-bar-style" content="#f8f5ec">


<meta name="author" content="SongLei" /><meta name="description" content="最近在投了字节和网易的强化学习岗位，面试前复习了一遍强化学习和深度强化学习的基础知识，简单总结一下复习的内容以及面试的经验。
" /><meta name="keywords" content="AI, Sim2Real" />






<meta name="generator" content="Hugo 0.85.0 with theme even" />


<link rel="canonical" href="https://songlei.me/post/ai/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E9%9D%A2%E8%AF%95%E5%87%86%E5%A4%87/" />
<link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
<link rel="manifest" href="/manifest.json">
<link rel="mask-icon" href="/safari-pinned-tab.svg" color="#5bbad5">



<link href="/sass/main.min.f92fd13721ddf72129410fd8250e73152cc6f2438082b6c0208dc24ee7c13fc4.css" rel="stylesheet">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.1.20/dist/jquery.fancybox.min.css" integrity="sha256-7TyXnr2YU040zfSP+rEcz29ggW4j56/ujTPwjMzyqFY=" crossorigin="anonymous">


<meta property="og:title" content="强化学习面试准备" />
<meta property="og:description" content="最近在投了字节和网易的强化学习岗位，面试前复习了一遍强化学习和深度强化学习的基础知识，简单总结一下复习的内容以及面试的经验。" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://songlei.me/post/ai/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E9%9D%A2%E8%AF%95%E5%87%86%E5%A4%87/" /><meta property="article:section" content="post" />
<meta property="article:published_time" content="2021-04-02T00:00:00+00:00" />
<meta property="article:modified_time" content="2021-04-02T00:00:00+00:00" />

<meta itemprop="name" content="强化学习面试准备">
<meta itemprop="description" content="最近在投了字节和网易的强化学习岗位，面试前复习了一遍强化学习和深度强化学习的基础知识，简单总结一下复习的内容以及面试的经验。"><meta itemprop="datePublished" content="2021-04-02T00:00:00+00:00" />
<meta itemprop="dateModified" content="2021-04-02T00:00:00+00:00" />
<meta itemprop="wordCount" content="3996">
<meta itemprop="keywords" content="人工智能,强化学习," /><meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="强化学习面试准备"/>
<meta name="twitter:description" content="最近在投了字节和网易的强化学习岗位，面试前复习了一遍强化学习和深度强化学习的基础知识，简单总结一下复习的内容以及面试的经验。"/>

<!--[if lte IE 9]>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/classlist/1.1.20170427/classList.min.js"></script>
<![endif]-->

<!--[if lt IE 9]>
  <script src="https://cdn.jsdelivr.net/npm/html5shiv@3.7.3/dist/html5shiv.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/respond.js@1.4.2/dest/respond.min.js"></script>
<![endif]-->

</head>
<body>
  <div id="mobile-navbar" class="mobile-navbar">
  <div class="mobile-header-logo">
    <a href="/" class="logo">Song Lei&#39;s blog</a>
  </div>
  <div class="mobile-navbar-icon">
    <span></span>
    <span></span>
    <span></span>
  </div>
</div>
<nav id="mobile-menu" class="mobile-menu slideout-menu">
  <ul class="mobile-menu-list">
    <a href="/">
        <li class="mobile-menu-item">Home</li>
      </a><a href="/post/">
        <li class="mobile-menu-item">Archives</li>
      </a><a href="/tags/">
        <li class="mobile-menu-item">Tags</li>
      </a><a href="/me/">
        <li class="mobile-menu-item">Me</li>
      </a>
  </ul>

  


</nav>

  <div class="container" id="mobile-panel">
    <header id="header" class="header">
        <div class="logo-wrapper">
  <a href="/" class="logo">Song Lei&#39;s blog</a>
</div>





<nav class="site-navbar">
  <ul id="menu" class="menu">
    <li class="menu-item">
        <a class="menu-item-link" href="/">Home</a>
      </li><li class="menu-item">
        <a class="menu-item-link" href="/post/">Archives</a>
      </li><li class="menu-item">
        <a class="menu-item-link" href="/tags/">Tags</a>
      </li><li class="menu-item">
        <a class="menu-item-link" href="/me/">Me</a>
      </li>
  </ul>
</nav>

    </header>

    <main id="main" class="main">
      <div class="content-wrapper">
        <div id="content" class="content">
          <article class="post">
    
    <header class="post-header">
      <h1 class="post-title">强化学习面试准备</h1>

      <div class="post-meta">
        <span class="post-time"> 2021-04-02 </span>
        <div class="post-category">
            <a href="/categories/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/"> 人工智能 </a>
            <a href="/categories/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/"> 强化学习 </a>
            </div>
        
      </div>
    </header>

    <div class="post-toc" id="post-toc">
  <h2 class="post-toc-title">文章目录</h2>
  <div class="post-toc-content always-active">
    <nav id="TableOfContents">
  <ul>
    <li><a href="#1-经典强化学习">1. 经典强化学习</a>
      <ul>
        <li><a href="#11-value-based">1.1 Value based</a></li>
        <li><a href="#12-policy-based">1.2 Policy based</a></li>
        <li><a href="#13-其他">1.3 其他</a></li>
      </ul>
    </li>
    <li><a href="#2-深度强化学习">2 深度强化学习</a>
      <ul>
        <li><a href="#21-value-based">2.1 Value based</a></li>
        <li><a href="#22-policy-based">2.2 Policy based</a></li>
      </ul>
    </li>
    <li><a href="#3-相关论文">3 相关论文</a></li>
    <li><a href="#4-总结">4 总结</a></li>
  </ul>
</nav>
  </div>
</div>
    <div class="post-content">
      <p>最近在投了字节和网易的强化学习岗位，面试前复习了一遍强化学习和深度强化学习的基础知识，简单总结一下复习的内容以及面试的经验。</p>
<hr>
<h1 id="1-经典强化学习">1. 经典强化学习</h1>
<p>这部分内容主要是Sutton的书上的内容的总结。</p>
<p>首先是基础的一些定义：</p>
<p>状态值函数：$v_{\pi}(s_t) = E_{\pi}[G_t | s_t] = E_{\pi}[R_{t+1}+\gamma G_{t+1} | s_t] = E_{\pi}[R_{t+1} + \gamma E_{\pi}[G_{t+1}|s_{t+1}] | s_t] = E_{\pi}[R_{t+1} + \gamma v_{\pi}(s_{t+1}) | s_t]$</p>
<p>状态动作值函数：$q_{\pi} (s_t, a_t) = E_{\pi}[G_t | s_t, a_t]$</p>
<p>二者的关系是：$v_{\pi}(s_t) = E_{a}[q(s_t, a)]$</p>
<p>最优状态值函数：$v_*(s) = \max_{\pi}v_{\pi}(s) = \max_a q_*(s, a)$</p>
<p>$v_*(s) = \max_a q_*(s, a) = \max_a E_{\pi_*}[R_{t+1} + \gamma v_*(s')|s, a]$</p>
<p>最优状态动作值函数：$q_*(s, a) = \max_\pi q_{\pi}(s, a) = E[R_{t+1} + \gamma v_*(s') | s, a]$</p>
<h2 id="11-value-based">1.1 Value based</h2>
<p>首先是Value based的方法，主要包括DP，TD，MC三大块。</p>
<ol>
<li>DP方法</li>
</ol>
<p>DP方法是一类优化方法，在已知模型(model based)的条件下，可以通过迭代的方式求解。主要有两种方法：策略迭代和值迭代。
策略迭代和值迭代的区别：策略迭代的收敛速度更快，只需要很少的更新次数就能收敛，但每步中都需要重复计算$v_{\pi}(s)$，计算量更大。而值迭代则相反，收敛慢，但计算量小。</p>
<p>1.1 策略迭代</p>
<p>策略迭代包括策略评估和策略改进，策略评估过程是通过迭代贝尔曼方程求解得到$v_\pi(s)$，然后通过$\pi'(s) = argmax_a q(s, a)$的方式优化策略，不断重复。</p>
<p>1.2 值迭代</p>
<p>值迭代是只进行一次策略评估，然后进行策略改进的策略迭代特例。值迭代优化的是贝尔曼最优方程。$v_{k+1} = \max_a E[R_{t+1} + \gamma v_k(s') | s, a]$</p>
<ol start="2">
<li>TD方法</li>
</ol>
<p>TD方法是通过$v(s_t) = v(s_t) + \alpha (R_{t+1} + \gamma v(s_{t+1}) - v(s_t))$进行更新的，有两个重要的方法，Sarsa和Q learning，他们的区别和更新公式要记住。</p>
<p>Sarsa优化的是状态动作值函数：$Q(s_t, a_t) = Q(s_t, a_t) + \alpha (R_{t+1} + \gamma Q(s_{t+1}, a_{t+1} ) - Q(s_t, a_t))$</p>
<p>Q learning优化的是最优状态动作值函数$Q(s_t, a_t) = Q(s_t, a_t)+ \alpha (R_{t+1} + \max_a Q(s_{t+1}, a) - Q(s_t, a_t))$</p>
<p>区别是优化目标不同，一个是状态动作值函数，一个是最优状态动作值函数；一个on policy，一个off policy；一个更加谨慎，一个会选择更加危险但回报更多的路径。</p>
<p>这里还需要知道on policy和off policy的区别，什么是重要性采样。</p>
<ol start="3">
<li>
<p>MC方法，通过采样对状态进行估计，优点是不需要模型，从经验中学习，计算一个状态的估计的代价与总状态个数无关(所以只计算某一个状态的值时十分好用)，在马尔科夫性不成立时损失更小(因为没有使用bootstraping)。</p>
</li>
<li>
<p>三种方法的比较</p>
</li>
</ol>
<p>DP方法是model based，bootstraping，不需要从采样中学习，无偏的。</p>
<p>TD是model free，bootstraping，从采样中学习，有偏，方差小，下一步就可以更新。</p>
<p>MC是model free，不是bootstraping，从采样中学习，无偏，方差大，必须等到一个episode结束才能更新。</p>
<ol start="5">
<li>
<p>Model based和model free的结合&ndash;Dyna</p>
</li>
<li>
<p>TD和MC的折中&ndash;n步TD和TD( $\lambda$ )</p>
</li>
</ol>
<h2 id="12-policy-based">1.2 Policy based</h2>
<p>Sutton书中所讲的policy based部分比较少，经典强化学习内容主要包括策略梯度定理的证明，RINFORCE算法，baseline的使用。</p>
<p>1.2.1 策略梯度简介</p>
<p>策略梯度方法是直接优化策略，这种方法的优点是可以应用在连续动作空间；动作的概率变化更加平滑，有更好的收敛性保证。</p>
<p>1.2.2 策略梯度定理证明</p>
<p>这部分证明主要是参考SpinningUp中的证明过程。</p>
<p>$$ \nabla_{\theta}J(\pi_\theta) = \nabla_{\theta} E_{\tau\sim \pi_{\theta}}[R(\tau)] $$
$$ = \nabla_{\theta} \int p(\tau)R(\tau) $$
$$ =  \int \nabla_{\theta}p(\tau)R(\tau) $$
$$ = \int p(\tau)\nabla_{\theta}\log(p(\tau))R(\tau) $$
$$ = E_{\tau\sim \pi_{\theta}}[\nabla_{\theta}\log(p(\tau))R(\tau)] $$
$$ = E_{\tau\sim \pi_{\theta}[\nabla_{\theta}\log(p(s_0)\prod_{i=0} \pi_\theta({a_i|s_i}) p(s_{i+1}|s_i)) R(\tau)]} $$
$$ = E_{\tau \sim \pi_{\theta} [\sum_{i=0}\nabla_{\theta}\log(\pi_{\theta}(a_i|s_i)) R(\tau)]} $$</p>
<p>1.2.3 RINFORCE</p>
<p>REINFORCE算法是通过蒙特卡洛采样的方法获得上面的$R(\tau)$的方法，这种方法是更新梯度的无偏估计，但方差较大(因为需要与环境交互到终止，每次交互获得的奖赏$R_t$都是一个随机变量，求方差时每个方差会累加)。</p>
<p>改进方法是使用TD方法，通过少量的偏差换取更小的方差(这部分方法在后文的深度强化学习中讲述)；或者加入baseline，这种方法在不引入偏差的同时，减小了方差。</p>
<p>1.2.4 Baseline的作用及相关证明</p>
<p>在原来的式子上加入baseline而不会引入偏差的原因是：$E[\nabla_{\theta}\log(\pi_{\theta}(x)) b(s_t)] = 0$，证明如下：</p>
<p>$$\int \pi(a_t|s_t) = 1$$</p>
<p>两边求导：</p>
<p>$$\nabla_{\theta}\int \pi(a_t|s_t) = \int \nabla_{\theta}\pi(a_t|s_t) = \int \pi(a_t|s_t)\nabla_{\theta}\log(\pi(a_t|s_t)) = E[\nabla_{\theta}\log(\pi(a_t|s_t))] = 0$$</p>
<p>Baseline是与$\theta$无关的常数，所以乘上后依然为0。</p>
<h2 id="13-其他">1.3 其他</h2>
<p>蒙特卡洛树搜索似乎也可能会问：包括选择，扩展，rollout，回溯四步，要会简单讲一下四步都是什么。</p>
<h1 id="2-深度强化学习">2 深度强化学习</h1>
<h2 id="21-value-based">2.1 Value based</h2>
<p>这部分内容主要是DQN及其变种(共有六种改进方式，集大成者为Rainbow)。</p>
<p>DQN：</p>
<p>DQN的更新公式：$E[(R_{t+1} + \gamma \max_a Q_{target}(s_{t+1}, a) - Q(s_t, a_t))^2]$，对损失函数求导就可以得到更新公式。</p>
<p>改进1. Double DQN</p>
<p>对Q函数过估计问题进行了改善。</p>
<p>改进2. Duel DQN</p>
<p>对网络结构进行改进，将Q分为V和Advantage，网络分别输出V和Advantage，然后计算$Q = V + (A - meanA)$。这种改进有效果直观上是因为可以判断某个状态动作值估值高，是因为这个状态本身就好，所以估值高，还是因为动作好，所以估值高。需要减去A的均值是因为如果没有任何限制，则对于同一个Q，可以有任意多的V和A的组合，可能出现V和Q值基本相等，A等于0的情况，约等于网络结构没有任何改进，所以通过对A增加限制，迫使网络学到准确的V值和A值。</p>
<p>改进3. Priority Replay</p>
<p>根据TD error赋予每个样本重要性权值，优先选择TD error大(改进明显)的样本。</p>
<p>改进4. Distributed DQN</p>
<p>不懂。。。</p>
<p>改进5. Noisy DQN</p>
<p>为网络参数增加噪声，提高探索能力。</p>
<p>改进6. n step DQN</p>
<h2 id="22-policy-based">2.2 Policy based</h2>
<p>A2C</p>
<ol start="2">
<li>A3C</li>
</ol>
<p>A3C通过异步的方式，降低数据的相关性。实现异步的方式是并行。</p>
<ol start="3">
<li>
<p>DDPG</p>
</li>
<li>
<p>D4PG</p>
</li>
<li>
<p>TD3(因为简历中写了个复现过TD3，所以被问了很多，这里的总结也比较多)</p>
</li>
</ol>
<p>TD3是DDPG的改进版本，主要是讨论了通过减小函数近似误差的方式，减缓AC框架中的过估计问题和高方差问题。DDPG的问题在于Q learning本身max操作的影响，会引入overestimate；TD方法使用了下一个状态的值估计来改善当前的值估计，而下一个状态的值估计往往是不准确的，不精确的估计(也就是对Q函数估计的不准确)会导致TD更新中误差的累计。</p>
<p>所以TD3提出两个重要的改进：T(Twin)是使用两个Q网络中较小的一个进行估值，可以减缓overestimate的问题；D(Delayed)是延迟更新，延迟更新的是actor，target actor，target critic网络，延迟策略更新能够减少每次更新的误差累计问题，改进性能。还有一个DQN之后已经广泛使用的trick：目标网络，通过减少近似Q函数的误差，减少方差(其他作用还有提供稳定的更新目标，减少数据的相关性、使训练更加稳定)。</p>
<ol start="6">
<li>SAC</li>
</ol>
<p>SAC是基于Max entropy reinforcement learning的一种算法，实现比较简单，难点是理解Soft Bellman Equation和Soft Q。Max entropy reinforcement learning主要是可以解决策略函数存在多峰的情况，一种比较简单的方法是将策略建模为玻尔兹曼分布$\pi(a_t|s_t) \propto exp(Q(s_t, a_t))$。而论文中证明了通过优化Soft bellman equation求得的最优策略，就是服从玻尔兹曼分布的策略，所以通过优化Soft bellman equation，就可以得到策略。</p>
<ol start="7">
<li>TRPO和PPO</li>
</ol>
<p>TRPO和PPO的优化目标，以及求解TRPO时使用的共轭梯度法。</p>
<h1 id="3-相关论文">3 相关论文</h1>
<p>因为面试的是网易的游戏AI职位，所以这几天也稍微看了几篇比较经典的游戏AI论文，主要是围棋和王者荣耀，方便与面试官交流(似乎没有看到网易的游戏AI论文。。。 )。</p>
<p>3.1 Mastering the game of Go with deep neural networks and tree search</p>
<p>AlphaGo的论文。围棋游戏的难点在于巨大的搜索空间和棋局(状态)难以评估。AlphaGo的思路是将MCTS和NN，减少搜索树的深度和宽度。</p>
<p>AlphaGo中包括四个网络：从人类专家数据训练的监督学习策略网络、快速选择的rollout网络；使用SL策略网络初始化并与历史版本的网络自我对弈训练的RL策略网络；RL策略网络自我对弈训练(可以有效减缓过拟合)的值网络。
然后利用RL策略网络，rollout网络，值网络配合MCTS算法。</p>
<p>3.2 Mastering Complex Control in MOBA Games with Deep Reinforcement Learning</p>
<p>王者荣耀论文。该论文主要是解决1v1游戏中的复杂控制问题，在system和algorithm两个方面做出了改进。
系统设计方面，系统包括RL Learner(分布式强化学习环境)，AI Server(分布在多个CPU上，通过selfplay的方式与环境交互)，Dispatch Module(每个Dispatch与多个AI Server在同一个机器上，收集AI Server的样本，压缩打包发送给Memory Pool)，Memory Pool(存储收集的样本数据)四个组件。</p>
<p>算法方面，主要是RL Learner的实现，使用了AC框架，状态输入为局部的图像信息(英雄、小兵、防御塔)、图像的种类(英雄、生命值等)、游戏信息(当前时间等)，动作主要是移动方向和目标。使用了一些tricks：target attention(用特定模块选择攻击目标)、LSTM(策略网络加入LSTM学习连招)、action mask(在特定状态下，引入先验知识减少动作选择的维度)、multi label dual cliped PPO(多个动作独立考虑，并且使用$E_t[max(min(r_t(\theta)A_t, clip(r_t(\theta), 1-\epsilon, 1+\epsilon)A_t), cA_t)]$)，限制 $r_t(\theta)$ 的下界。</p>
<hr>
<h1 id="4-总结">4 总结</h1>
<p>面试的感受是准备应该更加充分、深入，尤其是写在简历上的东西，要深入了解，比如TD3，不仅要了解其算法思路，比如T是什么，D是什么等等，还要理解为什么这样做，比如减少方差偏差，以及为什么会有方差偏差等等。在问完一些基础知识后，面试官通常就会开始深入问简历上的内容。</p>
<p>虽然大部分问题都是强化学习相关，但也会问到一些基础的编程方便的知识，所以编程基础还是要牢固。</p>
<p>最后，希望我面试顺利。。。</p>
    </div>

    <div class="post-copyright">
  <p class="copyright-item">
    <span class="item-title">文章作者</span>
    <span class="item-content">SongLei</span>
  </p>
  <p class="copyright-item">
    <span class="item-title">上次更新</span>
    <span class="item-content">
        2021-04-02
        
    </span>
  </p>
  
  
</div>
<footer class="post-footer">
      <div class="post-tags">
          <a href="/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/">人工智能</a>
          <a href="/tags/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/">强化学习</a>
          </div>
      <nav class="post-nav">
        <a class="prev" href="/post/website/%E7%BD%91%E7%AB%99%E5%BA%95%E9%83%A8%E6%B7%BB%E5%8A%A0%E8%BF%90%E8%A1%8C%E6%97%B6%E9%97%B4/">
            <i class="iconfont icon-left"></i>
            <span class="prev-text nav-default">网站底部添加运行时间</span>
            <span class="prev-text nav-mobile">上一篇</span>
          </a>
        <a class="next" href="/post/code/python%E7%88%AC%E8%99%AB%E5%88%B7%E5%8D%9A%E5%AE%A2%E8%AE%BF%E9%97%AE%E9%87%8F/">
            <span class="next-text nav-default">Python爬虫刷博客访问量</span>
            <span class="next-text nav-mobile">下一篇</span>
            <i class="iconfont icon-right"></i>
          </a>
      </nav>
    </footer>
  </article>
        </div>
        

  

  

      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="social-links">
      <a href="mailto:your@email.com" class="iconfont icon-email" title="email"></a>
      <a href="https://github.com/s974534426" class="iconfont icon-github" title="github"></a>
  <a href="https://songlei.me/index.xml" type="application/rss+xml" class="iconfont icon-rss" title="rss"></a>
</div>

<div>
  <center>本站已经开心运行<span id="runtime_span"></span></center>
  
  <script type="text/javascript">
    function show_runtime() {
      window.setTimeout("show_runtime()", 1000);
      start_time = new Date("03/01/2021 00:00:00");
      cur_time = new Date();
      run_time = cur_time.getTime()-start_time.getTime();
      TIME_PER_DAY=24*60*60*1000;
      day = run_time / TIME_PER_DAY;
      floor_day = Math.floor(day);
      hour = (day - floor_day) * 24;
      floor_hour = Math.floor(hour);
      minute = (hour - floor_hour) * 60;
      floor_minute = Math.floor(minute);
      floor_second = Math.floor((minute - floor_minute)*60);
      runtime_span.innerHTML = floor_day+"天"+floor_hour+"小时"+floor_minute+"分"+floor_second+"秒"
    } show_runtime();
  </script>
</div>

<div class="copyright">
  <span class="power-by">
    由 <a class="hexo-link" href="https://gohugo.io">Hugo</a> 强力驱动
  </span>
  <span class="division">|</span>
  <span class="theme-info">
    主题 - 
    <a class="theme-link" href="https://github.com/olOwOlo/hugo-theme-even">Even</a>
  </span>

  

  <span class="copyright-year">
    &copy; 
    2020 - 
    2021<span class="heart"><i class="iconfont icon-heart"></i></span><span>Song Lei</span>
  </span>
</div>






    </footer>

    <div class="back-to-top" id="back-to-top">
      <i class="iconfont icon-up"></i>
    </div>
  </div>
  
  <script src="https://cdn.jsdelivr.net/npm/jquery@3.2.1/dist/jquery.min.js" integrity="sha256-hwg4gsxgFZhOsEEamdOYGBf13FyQuiTwlAQgxVSNgt4=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/slideout@1.0.1/dist/slideout.min.js" integrity="sha256-t+zJ/g8/KXIJMjSVQdnibt4dlaDxc9zXr/9oNPeWqdg=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.1.20/dist/jquery.fancybox.min.js" integrity="sha256-XVLffZaxoWfGUEbdzuLi7pwaUJv1cecsQJQqGLe7axY=" crossorigin="anonymous"></script>



<script type="text/javascript" src="/js/main.min.c99b103c33d1539acf3025e1913697534542c4a5aa5af0ccc20475ed2863603b.js"></script>
  <script type="text/javascript">
    window.MathJax = {
      tex: {
        inlineMath: [['$','$'], ['\\(','\\)']],
        tags: 'ams',
        }
    };
  </script>
  <script async src="https://cdn.jsdelivr.net/npm/mathjax@3.0.5/es5/tex-mml-chtml.js" integrity="sha256-HGLuEfFcsUJGhvB8cQ8nr0gai9EucOOaIxFw7qxmd+w=" crossorigin="anonymous"></script>








</body>
</html>
